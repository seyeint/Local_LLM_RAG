# Local google's GEMMA LLM RAG build.

![image](https://github.com/seyeint/Local_LLM_RAG/assets/36778187/980eb5f9-6b08-44e9-9f90-2f5e32aa68b4)

## Usage
Using this local RAG and hopefully making it better as new models come out or even better quantization strategies allow me to use >7B parameter models in my 3080.
Notebook is annotated with everything one needs to do it alone and refine it. 
