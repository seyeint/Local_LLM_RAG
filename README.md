![image](https://github.com/seyeint/Local_LLM_RAG/assets/36778187/d592f9c1-fab8-453e-a430-49185b380084)

Using this local RAG and hopefully making it better as new models come out or even better quantization strategies allow me to use >7B parameter models in my 3080.
Notebook is annotated with everything one needs to do it alone and refine it. 
